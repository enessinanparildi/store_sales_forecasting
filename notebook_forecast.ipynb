{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85da2a99-3da6-4167-823d-7dd7d65d3395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\eness\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\eness\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: catboost in c:\\users\\eness\\anaconda3\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: icecream in c:\\users\\eness\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\eness\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\eness\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\eness\\anaconda3\\lib\\site-packages (0.14.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\eness\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\eness\\anaconda3\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: scipy in c:\\users\\eness\\anaconda3\\lib\\site-packages (from catboost) (1.13.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\eness\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\eness\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from icecream) (2.15.1)\n",
      "Requirement already satisfied: executing>=2.1.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from icecream) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from icecream) (2.0.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\eness\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy catboost icecream matplotlib seaborn statsmodels scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902772e0-61ab-441a-b1f6-264ee6730b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from icecream import ic\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d78c294-2fae-4cdb-95c9-40bd5f5fbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "matplotlib.use(\"Agg\")  # disables GUI plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce54d17-9e65-4d22-8a1f-da2988de92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(\"./data/dataset.csv\")\n",
    "promotions_df = pd.read_csv(\"./data/promotions.csv\")\n",
    "stores_df = pd.read_csv(\"./data/stores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d56fe-d3f4-4cf4-845a-a954981dc55d",
   "metadata": {},
   "source": [
    "First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff8736f-6493-4be9-9b9e-035db3dd7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptypromosinceweek = len(stores_df['PromoSinceWeek'][stores_df['PromoSinceWeek'].isna()])\n",
    "emptypromosinceyear = len(stores_df['PromoSinceYear'][stores_df['PromoSinceYear'].isna()])\n",
    "emptyPromoInterval = len(stores_df['PromoInterval'][stores_df['PromoInterval'].isna()])\n",
    "emptycomptdistance = len(stores_df['CompetitionDistance'][stores_df['CompetitionDistance'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1db585-e4d0-4726-b8ac-1ab19f3e501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "print(emptypromosinceweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f973ff-8698-4904-a967-f2f78bbdfbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "544\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(emptypromosinceyear)\n",
    "print(emptyPromoInterval)\n",
    "print(emptycomptdistance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f4807d6-8265-4c59-8650-bbc83f9e3d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115\n",
      "1115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assortment</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large</td>\n",
       "      <td>5.568195e+06</td>\n",
       "      <td>5.736887e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>7.882924e+06</td>\n",
       "      <td>1.885836e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Small</td>\n",
       "      <td>4.967538e+06</td>\n",
       "      <td>5.611584e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Assortment         Sales     Customers\n",
       "0      Large  5.568195e+06  5.736887e+05\n",
       "1     Medium  7.882924e+06  1.885836e+06\n",
       "2      Small  4.967538e+06  5.611584e+05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Question\n",
    "result_df_total_sales_customers = dataset_df.groupby('Store')[['Sales', 'Customers']].sum().reset_index()\n",
    "print(len(result_df_total_sales_customers))\n",
    "print(len(stores_df))\n",
    "merged_data = pd.merge(result_df_total_sales_customers, stores_df, on='Store', how='inner')\n",
    "merged_data = merged_data[['Store', \"Sales\", \"Customers\", \"Assortment\"]].copy()\n",
    "mean_by_assortment = merged_data.groupby('Assortment')[['Sales', 'Customers']].mean().reset_index()\n",
    "mean_by_assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391c69c0-3272-4b97-99b8-81293399d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# Third Question\n",
    "dataset_df['Date'] = pd.to_datetime(dataset_df['Date'])\n",
    "dataset_df_2014 = dataset_df[dataset_df['Date'].dt.year == 2014]\n",
    "dataset_df_2014_sorted = dataset_df_2014.sort_values(by='Sales', ascending=False)\n",
    "print(dataset_df_2014_sorted.iloc[0]['Store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28dc4f9e-d1d2-466f-979f-b732765617ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01</td>\n",
       "      <td>4429.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-02</td>\n",
       "      <td>4629.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-03</td>\n",
       "      <td>5221.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-04</td>\n",
       "      <td>4675.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-05</td>\n",
       "      <td>4849.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-03</td>\n",
       "      <td>5429.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-04</td>\n",
       "      <td>5623.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-05</td>\n",
       "      <td>5520.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-06</td>\n",
       "      <td>5543.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-07</td>\n",
       "      <td>5581.370370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store     Date        Sales\n",
       "0         2  2013-01  4429.653846\n",
       "1         2  2013-02  4629.750000\n",
       "2         2  2013-03  5221.000000\n",
       "3         2  2013-04  4675.120000\n",
       "4         2  2013-05  4849.125000\n",
       "...     ...      ...          ...\n",
       "3195   1104  2015-03  5429.961538\n",
       "3196   1104  2015-04  5623.875000\n",
       "3197   1104  2015-05  5520.086957\n",
       "3198   1104  2015-06  5543.960000\n",
       "3199   1104  2015-07  5581.370370\n",
       "\n",
       "[3200 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fourth Question\n",
    "dataset_df['Date'] = pd.to_datetime(dataset_df['Date'])\n",
    "stores_df_filtered = stores_df[\n",
    "    (stores_df['CompetitionDistance'] < 1000) &\n",
    "    (stores_df['PromoInterval'] == 'Jan,Apr,Jul,Oct')\n",
    "]\n",
    "dataset_df_grouped = dataset_df.groupby([\"Store\", dataset_df['Date'].dt.to_period('M')])['Sales'].mean().reset_index()\n",
    "merged_data = pd.merge(stores_df_filtered, dataset_df_grouped,  on='Store', how='left')\n",
    "merged_data = merged_data[['Store', 'Date', 'Sales']].copy()\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b64a0b-004b-417a-8449-fb45c456b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for plotting\n",
    "\n",
    "\n",
    "# **ACF**\n",
    "# Measures correlation between the series and its lagged versions **including indirect effects**.\n",
    "\n",
    "# ## **PACF**\n",
    "# Measures **direct** correlation between the series and its lag at k **after removing effects of all intermediate lags**.\n",
    "def plot_acf_pacf(df, store_id, max_lag=60):\n",
    "    df = df[df[\"empty_store_flag\"] == 0]\n",
    "    s = (df.loc[df[\"Store\"] == store_id, [\"Date\", \"Sales\"]].sort_values(\"Date\").set_index(\"Date\")[\"Sales\"])\n",
    "\n",
    "    s = s.asfreq(\"D\")\n",
    "    s_clean = s.dropna()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "    plot_acf(s_clean, lags=max_lag, ax=axes[0])\n",
    "    plot_pacf(s_clean, lags=max_lag, ax=axes[1], method=\"ywm\")\n",
    "    axes[0].set_title(\"ACF\")\n",
    "    axes[1].set_title(\"PACF\")\n",
    "    fig.savefig(f\"acf_pacf_plot_{store_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "def plot_sales_hist(df, store_id):\n",
    "    df = df[df[\"empty_store_flag\"] == 0]\n",
    "    s = df.loc[df[\"Store\"] == store_id, [\"Date\", \"Sales\"]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(s['Sales'], bins=40, kde=False, color='skyblue', ax=ax)\n",
    "    ax.set_title(f'Sales Distribution - Store {store_id}')\n",
    "    ax.set_xlabel('Sales')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    fig.savefig(f\"sales_histogram_{store_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_seasonal_decompose(df, store_id):\n",
    "    df = df[df[\"empty_store_flag\"] == 0]\n",
    "    s = df.loc[df[\"Store\"] == store_id, [\"Date\", \"Sales\"]]\n",
    "    s = s['Sales'].astype(float)\n",
    "\n",
    "    #s = np.log1p(s['Sales'].astype(float))\n",
    "    trs = seasonal_decompose(s, model='additive', period=7)\n",
    "\n",
    "    # Check for heteroskedasticity\n",
    "    resid = trs.resid\n",
    "    resid = resid[~resid.isna()]\n",
    "    test_stat, p_value, _, _ = het_arch(resid)\n",
    "\n",
    "    # if < 0.05 Residuals are heteroskedastic (ARCH effect exists)\n",
    "    ic(store_id)\n",
    "    print(\"ARCH test statistic:\", test_stat)\n",
    "    print(\"p-value:\", p_value)\n",
    "\n",
    "    fig = trs.plot()  # capture the figure\n",
    "    fig.set_size_inches(12, 8)  # optional resize\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"non_log_seasonal_decompose_{store_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a81b3b-ed00-4f75-b0f3-3cb6a079cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling part\n",
    "# Reload the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22999414-7606-4308-b3c0-32686d467dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(\"./data/dataset.csv\")\n",
    "promotions_df = pd.read_csv(\"./data/promotions.csv\")\n",
    "stores_df = pd.read_csv(\"./data/stores.csv\")\n",
    "\n",
    "dataset_df['Date'] = pd.to_datetime(dataset_df['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5676fba-bff7-4654-82e5-c1e30285c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store\n",
       "1       DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "2       DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "3       DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "4       DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "5       DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "                              ...                        \n",
       "1111    DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "1112    DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "1113    DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "1114    DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "1115    DatetimeIndex(['2013-01-06', '2013-01-13', '20...\n",
       "Name: Date, Length: 1115, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get missing dates for each store\n",
    "def missing_dates(dates):\n",
    "    full = pd.date_range(dates.min(), dates.max(), freq='D')\n",
    "    return full.difference(dates)\n",
    "\n",
    "missing = dataset_df.groupby('Store')['Date'].apply(missing_dates)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6951d828-3cd1-425f-8281-17da4566c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eness\\AppData\\Local\\Temp\\ipykernel_53784\\3424360171.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_df = dataset_df.groupby('Store', group_keys=False).apply(fill_dates).reset_index().rename(columns={'index': 'Date'})\n"
     ]
    }
   ],
   "source": [
    "# Fill missing dates for each store and create empty store time flag\n",
    "def fill_dates(dates):\n",
    "    full_index = pd.date_range(start=dates[\"Date\"].min(), end=dates[\"Date\"].max(), freq='D')\n",
    "    dates = dates.set_index('Date').reindex(full_index)\n",
    "    return dates\n",
    "\n",
    "dataset_df = dataset_df.groupby('Store', group_keys=False).apply(fill_dates).reset_index().rename(columns={'index': 'Date'})\n",
    "dataset_df['Store'] = dataset_df['Store'].ffill()\n",
    "dataset_df['empty_store_flag'] = 0\n",
    "dataset_df.loc[dataset_df['Sales'].isna(), 'empty_store_flag'] = 1\n",
    "dataset_df[\"Sales\"] = dataset_df[\"Sales\"].fillna(0)\n",
    "dataset_df[\"Customers\"] = dataset_df[\"Customers\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d90dbc2-4f6b-4c2e-a15a-e7d2e9c1bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and merge daily spot promo flag\n",
    "promotions_df['spot_promo_flag'] = 1\n",
    "promotions_df['Date'] = pd.to_datetime(promotions_df['Date'])\n",
    "merged_data = pd.merge(dataset_df, promotions_df, on=['Store', \"Date\"], how='left')\n",
    "merged_data['spot_promo_flag'] = merged_data['spot_promo_flag'].fillna(0)\n",
    "merged_data_all = pd.merge(merged_data, stores_df, on='Store', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f8b93bc-72c4-4557-a107-5694ca3cccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cyclical promo flag\n",
    "def check_cyclical_promo(row):\n",
    "    month_map = {\n",
    "        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n",
    "        'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n",
    "        'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "\n",
    "    if pd.isna(row['PromoSinceYear']) or row['PromoSinceWeek'] == 0:\n",
    "        return 0\n",
    "\n",
    "    if pd.isna(row['PromoInterval']):\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        promo_start_week = int(row['PromoSinceWeek'])\n",
    "        promo_start_year = int(row['PromoSinceYear'])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    current_year = row['Date'].year\n",
    "    current_week = row['Date'].isocalendar()[1]\n",
    "    current_month = row['Date'].month\n",
    "\n",
    "    if current_year < promo_start_year:\n",
    "        return 0\n",
    "    elif current_year == promo_start_year and current_week < promo_start_week:\n",
    "        return 0\n",
    "    else:\n",
    "        promo_months = row['PromoInterval'].split(',')\n",
    "        promo_month_nums = [month_map.get(m.strip(), 0) for m in promo_months]\n",
    "\n",
    "        if current_month in promo_month_nums:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "merged_data_all['cyclical_promo_flag'] = merged_data_all.apply(check_cyclical_promo, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9d723a8-d4c3-4664-bd04-650ea22afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess competition distance\n",
    "merged_data_all['empty_comp_distance'] = 0\n",
    "merged_data_all.loc[merged_data_all['CompetitionDistance'].isna(), 'empty_comp_distance'] = 1\n",
    "merged_data_all['CompetitionDistance'] = merged_data_all['CompetitionDistance'].fillna(max(merged_data_all['CompetitionDistance']))\n",
    "merged_data_all['CompetitionDistance'] = np.log1p(merged_data_all['CompetitionDistance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a43e3226-7957-4255-9431-985072ce9fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 137.49207420472044\n",
      "p-value: 1.3752703166849213e-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 181.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 95.61905333153963\n",
      "p-value: 4.088052029681499e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 361.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 41.976754755285036\n",
      "p-value: 7.570429781347717e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 541.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 14.48917105025515\n",
      "p-value: 0.1518251131490177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 721.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 50.63220830183033\n",
      "p-value: 2.041617010257462e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 901.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 90.6697126378063\n",
      "p-value: 3.944480448796724e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: 1081.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH test statistic: 52.06154201182207\n",
      "p-value: 1.1115672232231243e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = merged_data_all[['Store', 'Date', \"Customers\", 'Sales', 'spot_promo_flag', 'CompetitionDistance',\n",
    "                            'cyclical_promo_flag', 'empty_comp_distance', \"empty_store_flag\"]]\n",
    "\n",
    "# Run plotting utilities and save them\n",
    "stores = features['Store'].unique()\n",
    "for s in stores[::180]:\n",
    "    tmp = features[features['Store'] == s].sort_values('Date')\n",
    "    tmp_ = tmp[tmp[\"empty_store_flag\"] == 0]\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(tmp_['Date'], tmp_['Sales'])\n",
    "    ax.set_title(f\"Store {s} – Sales Over Time\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Sales\")\n",
    "\n",
    "    fig.savefig(f\"plot_{s}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plot_acf_pacf(features, s, max_lag=60)\n",
    "    plot_sales_hist(features, s)\n",
    "    plot_seasonal_decompose(features, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c6ef701-1ac2-416a-ba11-91cea4367804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preliminary feature engineering steps\n",
    "features = features.sort_values([\"Store\",\"Date\"])\n",
    "\n",
    "features['assortment_code'] = merged_data_all['Assortment'].astype('category').cat.codes\n",
    "features['Store'] = features['Store'].astype(str)  \n",
    "\n",
    "features[\"weekday\"] = features[\"Date\"].dt.weekday\n",
    "features[\"is_weekend\"] = (features[\"weekday\"] >= 5).astype(int)\n",
    "features[\"day\"] = features[\"Date\"].dt.day\n",
    "features[\"month\"] = features[\"Date\"].dt.month\n",
    "features[\"year\"] = features[\"Date\"].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e8f433c-b97f-426f-b62b-90a0675c43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales lag features\n",
    "for L in [1,2,3,4,5,6, 7, 14, 21,  28, 35, 364]:\n",
    "    features[f\"Sales_lag_{L}\"] = features.groupby(\"Store\")[\"Sales\"].shift(L)\n",
    "    features[f\"Customer_lag_{L}\"] = features.groupby(\"Store\")[\"Customers\"].shift(L)\n",
    "\n",
    "features[\"Sales_lag_diff_1\"] =  features[\"Sales_lag_1\"] -  features[\"Sales_lag_7\"]\n",
    "features[\"Sales_lag_diff_2\"] =  features[\"Sales_lag_7\"] -  features[\"Sales_lag_14\"]\n",
    "\n",
    "features[\"spot_promo_x_weekend\"] = features[\"spot_promo_flag\"] * features[\"is_weekend\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57e13eb1-0f1d-4694-a2fb-90427aee3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier transform features, fourier features give our model a compact, smooth representation of repeating seasonality.\n",
    "g = features.groupby(\"Store\")\n",
    "features[\"t\"] = g[\"Date\"].transform(lambda x: (x - x.min()).dt.days)\n",
    "for k in [1, 2, 3]:\n",
    "    features[f\"sin_wk_{k}\"] = np.sin(2*np.pi *k * features[\"t\"] / 7)\n",
    "    features[f\"cos_wk_{k}\"] = np.cos(2*np.pi *k * features[\"t\"] / 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f073e03e-678d-41bc-a51e-ba3ca06463af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An exponentially weighted (EW) mean is a moving average where recent observations\n",
    "# get more weight and older observations get exponentially decreasing weight.\n",
    "features[\"ewma_7\"] = features.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.shift(1).ewm(span=7, adjust=False, min_periods=3).mean())\n",
    "features[\"ewma_28\"] = features.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.shift(1).ewm(span=28, adjust=False, min_periods=7).mean())\n",
    "\n",
    "# Exponentially weighted std (volatility), e.g., 7-day\n",
    "features[\"ewstd_7\"] = features.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.shift(1).ewm(span=7, adjust=False, min_periods=3).std())\n",
    "# Compute rolling averages and rolling standard deviations over 7-28 days, use at least 3 valid days\n",
    "for W in [7, 28, 56]:\n",
    "    features[f\"Sales_rollmean_{W}\"] = features.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.shift(1).rolling(W, min_periods=3).mean())\n",
    "    features[f\"Sales_rollstd_{W}\"] = features.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.shift(1).rolling(W, min_periods=3).std())\n",
    "\n",
    "# A scale‑free momentum signal. By dividing “yesterday” by a recent average,\n",
    "# we tell the model whether the latest level is unusually high or low for this store right now,\n",
    "# without caring about the store’s absolute scale\n",
    "features[\"lag1_over_roll7\"] = features[\"Sales_lag_1\"] / (features[\"Sales_rollmean_7\"] + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47454249-73b2-4412-9de0-d6d257e1d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Store', 'spot_promo_flag', 'CompetitionDistance', \"empty_store_flag\",\n",
    "           'cyclical_promo_flag', 'empty_comp_distance', 'assortment_code', 'weekday',\n",
    "           'is_weekend', 'day', 'month', 'year', 'Sales_lag_1', 'Customer_lag_1', 'Sales_lag_2',\n",
    "           'Customer_lag_2', 'Sales_lag_3', 'Customer_lag_3', 'Sales_lag_4',\n",
    "           'Customer_lag_4', 'Sales_lag_5', 'Customer_lag_5', 'Sales_lag_6',\n",
    "           'Customer_lag_6', 'Sales_lag_7', 'Customer_lag_7', 'Sales_lag_14',\n",
    "           'Customer_lag_14', 'Sales_lag_21', 'Customer_lag_21', 'Sales_lag_28',\n",
    "           'Customer_lag_28', 'Sales_lag_35', 'Customer_lag_35', 'Sales_lag_364',\n",
    "           'Customer_lag_364', \"spot_promo_x_weekend\", \"ewma_7\", \"ewma_28\", \"ewstd_7\",\n",
    "           'Sales_rollmean_7', 'Sales_rollstd_7', 'Sales_rollmean_28',\n",
    "           'Sales_rollstd_28', 'Sales_rollmean_56', 'Sales_rollstd_56', \"Sales_lag_diff_1\", \"Sales_lag_diff_2\",\n",
    "            'sin_wk_1', 'cos_wk_1', 'sin_wk_2', 'cos_wk_2', 'sin_wk_3', 'cos_wk_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42a386fe-7af7-4d69-acca-8ec81454331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single train-validation split, optimize and tune the model using this validation split and calculate final metrics on another test split  \n",
    "cat_cols = [\"Store\",\"assortment_code\"]\n",
    "cat_idxs = [feature_cols.index(c) for c in cat_cols if c in feature_cols]\n",
    "\n",
    "cutoff = features[\"Date\"].quantile(0.9) # or a specific date\n",
    "train = features[(features[\"Date\"] <= cutoff)].dropna(subset=feature_cols + [\"Sales\"])\n",
    "valid = features[(features[\"Date\"] > cutoff)].dropna(subset=feature_cols + [\"Sales\"])\n",
    "\n",
    "X_tr, y_tr = train[feature_cols], train[\"Sales\"]\n",
    "X_va, y_va = valid[feature_cols], valid[\"Sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dad276a7-dc06-426b-a969-1dcdf98d1b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(y_tr): 538079\n",
      "ic| len(y_va): 104773\n"
     ]
    }
   ],
   "source": [
    "# Optinially, log transform stabilizes variance, reduces heteroskedasticity\n",
    "# Convert multiplicative effects to additive\n",
    "y_tr = np.log1p(y_tr)\n",
    "y_va = np.log1p(y_va)\n",
    "\n",
    "ic(len(y_tr))\n",
    "ic(len(y_va))\n",
    "\n",
    "pool_tr = Pool(X_tr, y_tr, cat_features=cat_idxs)\n",
    "pool_va = Pool(X_va, y_va, cat_features=cat_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62de391c-a6c1-4fdf-8a3c-182c99b581b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.5098904\ttest: 3.2450109\tbest: 3.2450109 (0)\ttotal: 479ms\tremaining: 55m 54s\n",
      "200:\tlearn: 0.1138996\ttest: 0.1176818\tbest: 0.1176818 (200)\ttotal: 1m 7s\tremaining: 37m 53s\n",
      "400:\tlearn: 0.1006818\ttest: 0.1125498\tbest: 0.1125498 (400)\ttotal: 2m 12s\tremaining: 36m 22s\n",
      "600:\tlearn: 0.0948425\ttest: 0.1109749\tbest: 0.1109496 (591)\ttotal: 3m 20s\tremaining: 35m 31s\n",
      "800:\tlearn: 0.0911643\ttest: 0.1102368\tbest: 0.1102218 (788)\ttotal: 4m 25s\tremaining: 34m 14s\n",
      "1000:\tlearn: 0.0885782\ttest: 0.1097005\tbest: 0.1096950 (996)\ttotal: 5m 31s\tremaining: 33m 7s\n",
      "1200:\tlearn: 0.0865641\ttest: 0.1091737\tbest: 0.1091669 (1196)\ttotal: 6m 36s\tremaining: 31m 53s\n",
      "1400:\tlearn: 0.0849646\ttest: 0.1089549\tbest: 0.1089549 (1400)\ttotal: 7m 41s\tremaining: 30m 44s\n",
      "1600:\tlearn: 0.0835268\ttest: 0.1086217\tbest: 0.1086191 (1599)\ttotal: 8m 46s\tremaining: 29m 34s\n",
      "1800:\tlearn: 0.0823253\ttest: 0.1083694\tbest: 0.1083405 (1791)\ttotal: 9m 51s\tremaining: 28m 26s\n",
      "2000:\tlearn: 0.0813122\ttest: 0.1080245\tbest: 0.1080245 (2000)\ttotal: 10m 55s\tremaining: 27m 17s\n",
      "2200:\tlearn: 0.0803795\ttest: 0.1078054\tbest: 0.1078018 (2197)\ttotal: 12m\tremaining: 26m 10s\n",
      "2400:\tlearn: 0.0795086\ttest: 0.1076496\tbest: 0.1076347 (2386)\ttotal: 13m 4s\tremaining: 25m 2s\n",
      "2600:\tlearn: 0.0787402\ttest: 0.1076540\tbest: 0.1075745 (2477)\ttotal: 14m 8s\tremaining: 23m 55s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.1075744946\n",
      "bestIteration = 2477\n",
      "\n",
      "Shrink model to first 2478 iterations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    iterations=7000,\n",
    "    learning_rate=0.04,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3.0,\n",
    "    subsample=0.8,\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    bootstrap_type='Bernoulli',\n",
    "    od_wait=200,\n",
    "    eval_metric=\"RMSE\",\n",
    "    task_type=\"CPU\",\n",
    "    verbose=200\n",
    "    )\n",
    "model.fit(pool_tr, eval_set=pool_va)\n",
    "predictions_validation = model.predict(pool_va)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ba934f7-29f4-4266-9c36-8145fcb6d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importance, this takes too long\n",
    "#    importance = model.get_feature_importance(data=pool_tr, type=\"LossFunctionChange\")\n",
    "#    fi = pd.DataFrame({\n",
    "#        'feature': model.feature_names_,\n",
    "#        'importance': importance\n",
    "#    }).sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5072a23e-2765-4157-9a0a-14fd81e41158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| model_rmse_error: 761.7067372905884\n",
      "ic| model_mae_error: 481.74724807668065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "481.74724807668065"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "yhat = np.expm1(predictions_validation)\n",
    "y_og = np.expm1(y_va)\n",
    "valid['predictions'] = yhat\n",
    "\n",
    "model_rmse_error = root_mean_squared_error(yhat, y_og)\n",
    "ic(model_rmse_error)\n",
    "\n",
    "# MAE is ideal when we want a straightforward measure of average error magnitude without emphasizing large errors disproportionately.\n",
    "model_mae_error = mean_absolute_error(yhat, y_og)\n",
    "ic(model_mae_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9437a73f-30a8-4939-a040-e923a88de15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| store_id: '1.0'\n",
      "ic| model_rmse_error: 370.0122108467618\n",
      "ic| store_id: '181.0'\n",
      "ic| model_rmse_error: 491.5619884700495\n",
      "ic| store_id: '361.0'\n",
      "ic| model_rmse_error: 629.2889803451918\n",
      "ic| store_id: '541.0'\n",
      "ic| model_rmse_error: 721.8251357376301\n",
      "ic| store_id: '721.0'\n",
      "ic| model_rmse_error: 693.3851971638824\n",
      "ic| store_id: '901.0'\n",
      "ic| model_rmse_error: 641.9562532150703\n",
      "ic| store_id: '1081.0'\n",
      "ic| model_rmse_error: 959.3812235499787\n"
     ]
    }
   ],
   "source": [
    "# Per Store Error\n",
    "stores = features['Store'].unique()\n",
    "for store_id in stores[::180]:\n",
    "    yhatfilter = valid[valid['Store'] == str(store_id)][\"predictions\"]\n",
    "    yogfilter =  valid[valid['Store'] == str(store_id)][\"Sales\"]\n",
    "    model_rmse_error = root_mean_squared_error(yhatfilter, yogfilter)\n",
    "    ic(store_id)\n",
    "    ic(model_rmse_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "523aa42f-6cef-42cc-b891-35bddb80a7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| naive_rmse_error_lag7: 3266.755906463662\n",
      "ic| naive_mae_error_lag7: 2175.0095062659275\n",
      "ic| naive_rmse_error_lag1: 4785.5996951762345\n",
      "ic| naive_mae_error_lag1: 3100.397631069073\n",
      "ic| naive_rmse_error_lag14: 2594.4611624330373\n",
      "ic| naive_mae_error_lag14: 1425.2945128993156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1425.2945128993156"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive baselines to compare, just output 7-1-14 days lagged sales value\n",
    "naive_rmse_error_lag7 = root_mean_squared_error(X_va['Sales_lag_7'], y_og)\n",
    "ic(naive_rmse_error_lag7)\n",
    "\n",
    "naive_mae_error_lag7 = mean_absolute_error(X_va['Sales_lag_7'], y_og)\n",
    "ic(naive_mae_error_lag7)\n",
    "\n",
    "naive_rmse_error_lag1 = root_mean_squared_error(X_va['Sales_lag_1'], y_og)\n",
    "ic(naive_rmse_error_lag1)\n",
    "\n",
    "naive_mae_error_lag1 = mean_absolute_error(X_va['Sales_lag_1'], y_og)\n",
    "ic(naive_mae_error_lag1)\n",
    "\n",
    "naive_rmse_error_lag14 = root_mean_squared_error(X_va['Sales_lag_14'], y_og)\n",
    "ic(naive_rmse_error_lag14)\n",
    "\n",
    "naive_mae_error_lag14 = mean_absolute_error(X_va['Sales_lag_14'], y_og)\n",
    "ic(naive_mae_error_lag14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b064904-8125-43ae-9a5f-a5b583512df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optinionally run random search hyperparameter optimization, we may setup a Bayesian hyperparameter optimization loop using ray tune library. \n",
    "# This is especially useful when multiple gpus are available  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6dc243e-ab4a-4b89-8bdd-8fb97fda1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_params(rng):\n",
    "    return {\n",
    "        \"depth\": int(rng.integers(6, 11)), # 6..10\n",
    "        \"learning_rate\": float(rng.uniform(0.01, 0.08)), \n",
    "        \"bagging_temperature\": float(rng.uniform(0.0, 1.0)),\n",
    "        \"subsample\": float(rng.uniform(0.6, 1.0)),\n",
    "        \"colsample_bylevel\": float(rng.uniform(0.6, 1.0)),\n",
    "        \"grow_policy\": rng.choice([\"SymmetricTree\", \"Lossguide\"]),\n",
    "        \"random_strength\": float(rng.uniform(0.0, 2.0)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3c9b6-292c-4399-8784-128023357945",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "seed = 25\n",
    "best = {\"rmse\": np.inf, \"params\": None, \"cv_scores\": None}\n",
    "\n",
    "for it in range(n_iter):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    params = sample_params(rng)\n",
    "    fold_rmses = []\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        depth=params[\"depth\"],\n",
    "        bagging_temperature=params[\"bagging_temperature\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bylevel=params[\"colsample_bylevel\"],\n",
    "        grow_policy=params[\"grow_policy\"],\n",
    "        random_strength=params[\"random_strength\"],\n",
    "        random_seed=seed,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=200,\n",
    "        use_best_model=True,\n",
    "        task_type=\"CPU\",\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model.fit(pool_tr, eval_set=pool_va, verbose=False)\n",
    "\n",
    "    # predict on validation, back-transform to original scale\n",
    "    yhat_log = model.predict(pool_va)\n",
    "    yhat = np.expm1(yhat_log)\n",
    "    y_va_og = np.expm1(y_va)\n",
    "    fold_rmses.append(root_mean_squared_error(y_va_og, yhat))\n",
    "\n",
    "    avg_rmse = float(np.mean(fold_rmses))\n",
    "    trials.append({\"iter\": it+1, \"params\": params, \"cv_rmse\": avg_rmse, \"folds\": fold_rmses})\n",
    "\n",
    "    if avg_rmse < best[\"rmse\"]:\n",
    "        best = {\"rmse\": avg_rmse, \"params\": params, \"cv_scores\": fold_rmses}\n",
    "\n",
    "    print(f\"[{it+1}/{n_iter}] RMSE={avg_rmse:.2f} best={best['rmse']:.2f} params={params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3c74b-f16d-4160-ab53-545430072f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
